---
title: "Class 8: PCR Mini Project"
author: "Amanda Wilpitz | A17463962"
format: pdf
---

Today we will do a complete analysis of some breast cancer biopsy data but first, let's revisit the main PCA function in R `prcomp()` and see what `scale=TRUE/FALSE` does. 

```{r}
head(mtcars)
```

Find the mean value per column of this dataset.

```{r}
apply(mtcars, 2, mean)
```

```{r}
apply(mtcars, 2, sd)
```

It is clear that "disp" and "hp" have the highest mean values and the higest standard deviation. They will likely dominate any analysis I do on this dataset. Let's see.

```{r}
pc.noscale <- prcomp(mtcars, scale = FALSE)
pc.scale <- prcomp(mtcars, scale=TRUE)
```


```{r}
biplot(pc.noscale)
```

```{r}
pc.noscale$rotation[,1]
```
Plot the loadings

```{r}
library(ggplot2)

r1 <- as.data.frame(pc.noscale$rotation)
r1$names <- rownames(pc.noscale$rotation)

ggplot(r1) + 
aes(PC1, names) + 
geom_col()
```

```{r}
r2 <- as.data.frame(pc.scale$rotation)
r2$names <- rownames(pc.scale$rotation)

ggplot(r2) + 
aes(PC1, names) + 
geom_col()
```

```{r}
biplot(pc.scale)
```

> **Take-home**: Generally, we always want to set `scale=TRUE` when we do this type of analysis to avoid our analysis being dominated by individual variables with the larges variance just due to their unit of measurement. 

# FNA Breast Cancer Data

Load the data into R. 

```{r}
fna.data <- read.csv("WisconsinCancer.csv")

wisc.df <- data.frame(fna.data, row.names=1)
```

```{r}
head(wisc.df)
```


> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(wisc.df$diagnosis == "M")
```
The `table()` function is super useful here:

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
ncol(wisc.df)
```

```{r}
colnames(wisc.df)
```

A useful function for this is `grep()`

```{r}
length( #Tells us how _means were found 
grep("_mean", colnames(wisc.df))) #Tells us which columns "_mean" were found
```

Before we go any further, we need to exclude the diagnosis column from any future analysis - this tells us whether a sample to cancer or non-cancer.

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)

head(diagnosis)
```

```{r}
wisc.data <- wisc.df[,-1]
```

Let's see if we cluster the `wisc.data` to find some structure in the dataset.

```{r}
hc <- hclust(dist(wisc.data))

plot(hc)
```

# Principal Component Analysis (PCA)

## Performing PCA

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)

summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

PC3

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

PC7

## Interpreting PCA Results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This biplot sucks! It is difficult to understand because there's too many data points to actually understand. 

We need to build our own PCA score plot of PC1 vs PC2. 

```{r}
attributes(wisc.pr)
```

```{r}
head(wisc.pr$x)
```

Plot of PC1 vs PC2 the first two columns
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis)
```

Let's rename the axis.

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis,
     xlab = "PC1", ylab = "PC2")
```

Make a ggplot version of this score plot

```{r}
pc <- as.data.frame(wisc.pr$x)

ggplot(pc) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

PCA compresses data into something that captures the essence of the original data -> takes a dataset with a lot of dimensions and flattens it into 2 or 3 dimensions so we can look at it. 

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

## Variance Explained

```{r}
#Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pr.var/sum(pr.var)
```


```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA Results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

PC5

# Hierarchal Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, "complete")
```


## Results of Hierarchal Clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

plot(wisc.hclust.clusters)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters_2 <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters_2, diagnosis)
```

```{r}
wisc.hclust.clusters_3 <- cutree(wisc.hclust, k=3)
table(wisc.hclust.clusters_3, diagnosis)
```

```{r}
wisc.hclust.clusters_6 <- cutree(wisc.hclust, k=6)
table(wisc.hclust.clusters_6, diagnosis)
```


```{r}
wisc.hclust.clusters_10 <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters_10, diagnosis)
```

None of them were good, since every clustering method did not have distinct benign and malignant grouping.

## Using Different Methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
wisc.hclust_ward.d2 <- hclust(data.dist, "ward.D2")

plot(wisc.hclust_ward.d2)

wisc.hclust.clusters_d2 <- cutree(wisc.hclust_ward.d2, k=3)
table(wisc.hclust.clusters_d2, diagnosis)
```

```{r}
wisc.hclust_single <- hclust(data.dist, "single")

plot(wisc.hclust_single)
```

I like "ward.D2" because when using it to cluster, it can cluster them into groups that are more only malignant or benign compared to before. 

## Clustering in PC Space

```{r}
head(wisc.pr$x[,1:3])
```

## K-means Clustering (setting variables for future sections)

```{r}
wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

# Combining Methods

```{r}
hc <- hclust(dist(wisc.pr$x[, 1:2]), method = "ward.D2")

plot(hc)
abline(h=70, col="red")
```

Cluster Membership Vector

```{r}
grps <- cutree(hc, h=70)
table(grps)
```
```{r}
table(diagnosis)
```

Cross-table to see how my clustering groups correspond to the expert diagnosis vector of M and B values

```{r}
table(grps, diagnosis)
```
Group 1 has mostly malignant and group 2 has mostly benign. 

Positive => cancer ("M")
Negative => non-cancerous ("B")

True = Cluster/Group 1 
False = Cluster/Group 2

True Positive: 177
False Positive: 18
True Negative: 339
False Negative: 35

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```


```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
wisc.pr.hclust.clusters_4 <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters_4)
```


```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters_4, diagnosis)
```
The newly created model with 4 clusters separates the two diagnoses out better, but the clusters still aren't distinctly only benign or malignant. 

>Q 16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.


```{r}
table(wisc.km$cluster, diagnosis)

table(wisc.hclust.clusters, diagnosis)
```

The k-means and hierarchical clustering models does a lot better in separating the diagnosis compared to previous sections. We can now have the data sorted into clusters of benign and malignant when before we only had the dendrogram clusters to base off of. 

# Sensitivity/Specificity

**Sensitivity** refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN).

**Specificity** relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FN).

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The ward.D2 clustering model has the best specificity. 

```{r}
#Specificity calculations for the k-means clustering model.
130/(130+82)

#Specificity calculations for the ward.D2 clustering model
165/(5+40+2+165)

```

The k-means clustering model has the best sensitivity. 

```{r}
#Sensitivity calculations model for the k-means clustering model
356/(356+1)

#Sensitivity calculations model for the ward.D2 clustering model
343/(343+2+12)
```

# Prediction

We can use our PCA results (wisc.pr) to make predictions on new unseen data. 

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize group 2 to follow up on based on our results. They are the group in the malignant diagnosis group. 
